# -*- coding: utf-8 -*-
"""ass2cv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k-So7DkloQ1dElHS_Q3X_OGRMz0C9Irk
"""

## Import necessary libraries
!pip install split-folders

import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from google.colab import drive
import pandas as pd
import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import os
import splitfolders
from sklearn.model_selection import train_test_split
import shutil
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Transformation for the training dataset with data augmentation
train_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# Transformation for validation and testing datasets
val_test_transform = transforms.Compose([
    transforms.Resize((256, 256)),                      # Resize images
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])



from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os
from torchvision import datasets, transforms
from torch.utils.data import DataLoader


train_dir = "/content/drive/MyDrive/food11/train"
val_dir = "/content/drive/MyDrive/food11/validation"
test_dir = "/content/drive/MyDrive/food11/test"



train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)
val_dataset = datasets.ImageFolder(root=val_dir, transform=val_test_transform)
test_dataset = datasets.ImageFolder(root=test_dir, transform=val_test_transform)


batch_size = 64  # Adjust batch size
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=4,
    pin_memory=True,
    prefetch_factor=2
)
val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=4,
    pin_memory=True,
    prefetch_factor=2
)
test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=False,       # No shuffling
    num_workers=4,
    pin_memory=True,
    prefetch_factor=2
)

print(f"Classes: {train_dataset.classes}")
print(f"Number of training images: {len(train_dataset)}")
print(f"Number of validation images: {len(val_dataset)}")
print(f"Number of test images: {len(test_dataset)}")

for images, labels in train_loader:
    print(f"Batch image shape: {images.shape}")
    print(f"Batch labels: {labels}")
    break

import torch
import torch.nn as nn
import torch.nn.functional as F

class CNNR(nn.Module):

    #Constructor
    def __init__(self, num_classes=10):
        super(CNNR, self).__init__()
        # 1. Convolutional Layer
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        # 2. Convolutional Layer
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        # 3. Convolutional Layer
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.res_conv2to3 = nn.Conv2d(64, 128, kernel_size=1)
        # 4. Convolutional Layer
        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)
        self.bn4 = nn.BatchNorm2d(256)
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)
        # 5. Convolutional Layer
        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)
        self.bn5 = nn.BatchNorm2d(512)
        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(512 * 16 * 16, 2048)    # first fully connected layer
        self.fc2 = nn.Linear(2048, num_classes)    # Second fully connected layer

    def forward(self, x):

        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.pool1(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.pool2(x)

        # Residual path
        residual = self.res_conv2to3(x)

        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)

        # Residual connection
        x = x + residual

        x = self.conv4(x)
        x = self.bn4(x)
        x = F.relu(x)
        x = self.pool4(x)

        x = self.conv5(x)
        x = self.bn5(x)
        x = F.relu(x)
        x = self.pool5(x)

        x = torch.flatten(x, 1)

        x = self.fc1(x)
        x = F.relu(x)

        x = self.fc2(x)

        return x

import torch
import torch.nn as nn
import torch.nn.functional as F

class CNN(nn.Module):
    def __init__(self, num_classes=10):
        super(CNN, self).__init__()

        # 1. Conv Block
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 2. Conv Block
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 3. Conv Block
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 4. Conv Block
        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)
        self.bn4 = nn.BatchNorm2d(256)
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 5. Conv Block
        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)
        self.bn5 = nn.BatchNorm2d(512)
        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Fully Connected Layers
        self.fc1 = nn.Linear(512 * 8 * 8, 512)

        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):
        # Conv Block 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.pool1(x)

        # Conv Block 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.pool2(x)

        # Conv Block 3
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.pool3(x)

        # Conv Block 4
        x = self.conv4(x)
        x = self.bn4(x)
        x = F.relu(x)
        x = self.pool4(x)

        # Conv Block 5
        x = self.conv5(x)
        x = self.bn5(x)
        x = F.relu(x)
        x = self.pool5(x)


        x = torch.flatten(x, 1)

        # Fully connected layers
        x = self.fc1(x)
        x = F.relu(x)
        #x = self.dropout(x)
        x = self.fc2(x)

        return x

## loss function
!pip install torchsummary
from torchsummary import summary

num_classes = 11
model = CNN(num_classes=num_classes)

criterion = nn.CrossEntropyLoss()


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

summary(model, (3, 256, 256))

# an optimizer
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.001,
    momentum=0.9,
    weight_decay=1e-4
)



import matplotlib.pyplot as plt
from tqdm import tqdm

def train_model(model,
                train_loader,
                val_loader,
                criterion,
                optimizer,
                device,
                num_epochs=50,
                save_path=None):
    """

    Returns:
        train_losses, val_losses, train_accuracies, val_accuracies
    """
    train_losses, val_losses = [], []
    train_accuracies, val_accuracies = [], []

    scaler = torch.cuda.amp.GradScaler()

    for epoch in range(num_epochs):
        print(f"Epoch {epoch + 1}/{num_epochs}")
        print("-" * 30)


        model.train()
        train_loss = 0
        correct = 0
        total = 0
        for images, labels in tqdm(train_loader):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()

            with torch.cuda.amp.autocast():  # automatic mixed precision
                outputs = model(images)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            train_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        train_loss /= len(train_loader.dataset)
        train_accuracy = 100.0 * correct / total
        train_losses.append(train_loss)
        train_accuracies.append(train_accuracy)
        print(f"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%")

        # --- Validation ---
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in tqdm(val_loader):
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * images.size(0)
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

        val_loss /= len(val_loader.dataset)
        val_accuracy = 100.0 * correct / total
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)
        print(f"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%")


    return train_losses, val_losses, train_accuracies, val_accuracies

"""##None resudual models with 32 batch sizes and 3 different learning rates

Model 1:

Training Hyperparameters:

>>Training Loss: 0.0268, Training Accuracy: 90.32%

>>Validation Loss: 1.6755, Validation Accuracy: 58.91%

Optimizer Type: SGD.

Learning Rate (lr): 0.001



Number of Epochs: 50

Batch Size:32

Loss Function:CrossEntropyLoss for multi-class classification tasks.


"""

train_losses, val_losses, train_accuracies, val_accuracies = train_model(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion,
    optimizer=optimizer,
    device=device,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_model.pth'
)

"""##None resudual
Model 1:

Training Hyperparameters:

>>Training Loss: 0.0450, Training Accuracy: 99.68%

>>Validation Loss: 2.3027, Validation Accuracy: 60.00%

>>Test Accuracy: 48.73%

Optimizer Type: SGD.

Learning Rate (lr): 0.001



Number of Epochs: 50

Batch Size:32

Loss Function:CrossEntropyLoss for multi-class classification tasks.

Dropout : (0.3)
"""

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

def evaluate_and_plot(model,
                      train_losses,
                      val_losses,
                      train_accuracies,
                      val_accuracies,
                      test_loader,
                      test_dataset,
                      device):


    # Plot Training and Validation Loss
    plt.figure(figsize=(12, 5))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.title('Loss Across Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Plot Training and Validation Accuracy
    plt.figure(figsize=(12, 5))
    plt.plot(train_accuracies, label='Training Accuracy')
    plt.plot(val_accuracies, label='Validation Accuracy')
    plt.title('Accuracy Across Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.grid(True)
    plt.show()

    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = outputs.max(1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Accuracy
    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))
    print(f"Test Accuracy: {accuracy * 100:.2f}%")

    # Classification Report
    class_names = test_dataset.classes
    print("\nClassification Report:\n")
    print(classification_report(all_labels, all_preds, target_names=class_names))

    # Confusion Matrix
    cm = confusion_matrix(all_labels, all_preds)

    # Plot Confusion Matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

evaluate_and_plot(model, train_losses, val_losses, train_accuracies, val_accuracies, test_loader, test_dataset, device)



"""Model 2:

Training Hyperparameters:

>>Training Loss: 0.0450, Training Accuracy: 99.36%

>>Validation Loss: 1.9283, Validation Accuracy: 55.64%

>>Test Accuracy: 49.45%


Optimizer Type: SGD.

Learning Rate (lr): 0.01



Number of Epochs: 50

Batch Size:32

Loss Function:CrossEntropyLoss for multi-class classification tasks.
"""

model2 = CNN(num_classes=num_classes)

criterion2 = nn.CrossEntropyLoss()

#model is on the CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model2.to(device)


optimizer2 = torch.optim.SGD(
    model2.parameters(),
    lr=0.01,

    weight_decay=1e-4
)

train_losses2, val_losses2, train_accuracies2, val_accuracies2 = train_model(
    model=model2,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion,
    optimizer=optimizer2,
    device=device,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_model2.pth'
)

evaluate_and_plot(model2, train_losses2, val_losses2, train_accuracies2, val_accuracies2, test_loader, test_dataset, device)

"""Model 3:


>>Training Loss: 0.0986, Training Accuracy: 98.64%

>>Validation Loss: 1.5711, Validation Accuracy:57.09%

>>Test Accuracy: 47.64%


Optimizer Type: SGD.

Learning Rate (lr): 0.005



Number of Epochs: 50

Batch Size:32

Loss Function:CrossEntropyLoss for multi-class classification tasks.
"""

model3 = CNN(num_classes=num_classes)

criterion3 = nn.CrossEntropyLoss()

#model is on the CPU
device3 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model3.to(device3)



optimizer3 = torch.optim.SGD(
    model3.parameters(),
    lr=0.005,

    weight_decay=1e-4
)

train_losses3, val_losses3, train_accuracies3, val_accuracies3 = train_model(
    model=model3,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion3,
    optimizer=optimizer3,
    device=device3,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_model3.pth'
)

evaluate_and_plot(model3, train_losses3, val_losses3, train_accuracies3, val_accuracies3, test_loader, test_dataset, device3)

"""
##CNN models with resudual  with 32 batch sizes and 3 different learning rates

Model 4:

Training Hyperparameters:

>>Training Loss: 0.3091, Training Accuracy:95.27%

>>Validation Loss: 1.5762,  Validation Accuracy: 49.82%

>>Test Accuracy: 45.*9%

Optimizer Type: SGD.

Learning Rate (lr): 0.001



Number of Epochs: 50

Batch Size:32

Loss Function:CrossEntropyLoss for multi-class classification tasks.



"""

model4 = CNNR(num_classes=num_classes)

criterion4 = nn.CrossEntropyLoss()

#model is on the CPU
device4 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model4.to(device4)



optimizer4 = torch.optim.SGD(
    model4.parameters(),
    lr=0.001,

    weight_decay=1e-4
)

train_losses4, val_losses4, train_accuracies4, val_accuracies4 = train_model(
    model=model4,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion4,
    optimizer=optimizer4,
    device=device4,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_model4.pth'
)

evaluate_and_plot(model4, train_losses4, val_losses4, train_accuracies4, val_accuracies4, test_loader, test_dataset, device4)

"""Model 5:

Training Hyperparameters:

>>Training Loss: 0.0238, Training Accuracy: 99.91%

>>Validation Loss: 1.7961,  Validation Accuracy: 54.91%

>>Test Accuracy: 47.27%

Optimizer Type: SGD.

Learning Rate (lr): 0.005



Number of Epochs: 50

Batch Size:32

Loss Function:CrossEntropyLoss for multi-class classification tasks.


"""

model5 = CNNR(num_classes=num_classes)

criterion5 = nn.CrossEntropyLoss()

#model is on the CPU
device5 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model5.to(device5)



optimizer5 = torch.optim.SGD(
    model5.parameters(),
    lr=0.005,

    weight_decay=1e-4
)

train_losses5, val_losses5, train_accuracies5, val_accuracies5 = train_model(
    model=model5,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion5,
    optimizer=optimizer5,
    device=device5,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_model5.pth'
)

evaluate_and_plot(model5, train_losses5, val_losses5, train_accuracies5, val_accuracies5, test_loader, test_dataset, device5)

"""Model 6:



Training Hyperparameters:

>>Training Loss: 0.0259, Training Accuracy:  99.36%

>>Validation Loss: 2.6958,  Validation Accuracy: 45.82%

>>Test Accuracy: 41.82%

Optimizer Type: SGD.

Learning Rate (lr): 0.01



Number of Epochs: 50

Batch Size:32

Loss Function:CrossEntropyLoss for multi-class classification tasks.


"""

model6 = CNNR(num_classes=num_classes)

criterion6 = nn.CrossEntropyLoss()

#model is on the CPU
device6 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model6.to(device6) # Move model to the correct device before calling summary



optimizer6 = torch.optim.SGD(
    model6.parameters(),
    lr=0.01,

    weight_decay=1e-4
)

train_losses6, val_losses6, train_accuracies6, val_accuracies6 = train_model(
    model=model6,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion6,
    optimizer=optimizer6,
    device=device6,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_model6.pth'
)

evaluate_and_plot(model6, train_losses6, val_losses6, train_accuracies6, val_accuracies6, test_loader, test_dataset, device6)

"""################################################################################

##None resudual models with 64 batch sizes and 3 different learning rates


Model 7:

Training Hyperparameters:

>>Training Loss: 0.1019, Training Accuracy: 98.55%

>>Validation Loss: 2.2136, Validation Accuracy: 46.55%

>>Test Accuracy: 40.73%

Optimizer Type: SGD.

Learning Rate (lr): 0.01



Number of Epochs: 50

Batch Size:64

Loss Function:CrossEntropy Loss for multi-class classification tasks.
"""

model7 = CNN(num_classes=num_classes)

criterion7 = nn.CrossEntropyLoss()

#model is on the CPU
device7 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model7.to(device7)



optimizer7 = torch.optim.SGD(
    model7.parameters(),
    lr=0.01,

    weight_decay=1e-4
)

train_losses7, val_losses7, train_accuracies7, val_accuracies7 = train_model(
    model=model7,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion7,
    optimizer=optimizer7,
    device=device7,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_model7.pth'
)

evaluate_and_plot(model7, train_losses7, val_losses7, train_accuracies7, val_accuracies7, test_loader, test_dataset, device7)

"""

Model 8:

Training Hyperparameters:

>>Training Loss: 0.2893, Training Accuracy: 93.14%

>>Validation Loss: 1.7062, Validation Accuracy: 46.91%

>>Test Accuracy: 47.27%

Optimizer Type: SGD.

Learning Rate (lr): 0.005



Number of Epochs: 50

Batch Size:64

Loss Function:CrossEntropy Loss for multi-class classification tasks."""

model8 = CNN(num_classes=num_classes)

criterion8 = nn.CrossEntropyLoss()

#model is on the CPU
device8 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model8.to(device8)

optimizer8 = torch.optim.SGD(
    model8.parameters(),
    lr=0.005,

    weight_decay=1e-4
)

train_losses8, val_losses8, train_accuracies8, val_accuracies8 = train_model(
    model=model8,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion8,
    optimizer=optimizer8,
    device=device8,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_model8.pth'
)

evaluate_and_plot(model8, train_losses8, val_losses8, train_accuracies8, val_accuracies8, test_loader, test_dataset, device8)

"""Model 9:

Training Hyperparameters:

>>Training Loss: 0.1252, Training Accuracy: 97.82%

>>Validation Loss: 2.6060, Validation Accuracy: 42.91%

>>Test Accuracy: 36.00%

Optimizer Type: SGD.

Learning Rate (lr): 0.001



Number of Epochs: 50

Batch Size:64

Loss Function:CrossEntropy Loss for multi-class classification tasks.
"""

model9 = CNN(num_classes=num_classes)

criterion9 = nn.CrossEntropyLoss()

#model is on the CPU
device9 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model9.to(device9)


optimizer9 = torch.optim.SGD(
    model9.parameters(),
    lr=0.001,

    weight_decay=1e-4
)

train_losses9, val_losses9, train_accuracies9, val_accuracies9 = train_model(
    model=model9,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion9,
    optimizer=optimizer9,
    device=device9,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_model9.pth'
)

evaluate_and_plot(model9, train_losses9, val_losses9, train_accuracies9, val_accuracies9, test_loader, test_dataset, device9)

"""##CNN Models with resudual  with 64 batch sizes and 3 different learning rates



Model 10:

Training Hyperparameters:

>>Training Loss: 0.0558, Training Accuracy: 99.41%

>>Validation Loss: 2.2422, , Validation Accuracy: 46.18%

>>Test Accuracy: 43.27%

Optimizer Type: SGD.

Learning Rate (lr): 0.01



Number of Epochs: 50

Batch Size:64

Loss Function:CrossEntropy Loss for multi-class classification tasks.
"""

model10 = CNNR(num_classes=num_classes)

criterion10 = nn.CrossEntropyLoss()

#model is on the CPU
device10 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model10.to(device10)


optimizer10 = torch.optim.SGD(
    model10.parameters(),
    lr=0.01,

    weight_decay=1e-4
)

train_losses10, val_losses10, train_accuracies10, val_accuracies10 = train_model(
    model=model10,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion10,
    optimizer=optimizer10,
    device=device10,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_model10.pth'
)

evaluate_and_plot(model10, train_losses10, val_losses10, train_accuracies10, val_accuracies10, test_loader, test_dataset, device10)

"""Model 11:

Validation Accuracy: 50.55%

Training Hyperparameters:

>>Training Loss:  0.1022, Training Accuracy: 98.86%

>>Validation Loss: 1.7752,  Validation Accuracy:50.55%

>>Test Accuracy: 40.73%

Optimizer Type: SGD.

Learning Rate (lr): 0.005



Number of Epochs: 50

Batch Size:64

Loss Function:CrossEntropy Loss for multi-class classification tasks.
"""

model11 = CNNR(num_classes=num_classes)

criterion11 = nn.CrossEntropyLoss()

#model is on the CPU
device11 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model11.to(device11)

optimizer11 = torch.optim.SGD(
    model11.parameters(),
    lr=0.005,

    weight_decay=1e-4
)

train_losses11, val_losses11, train_accuracies11, val_accuracies11 = train_model(
    model=model11,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion11,
    optimizer=optimizer11,
    device=device11,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_model11.pth'
)

evaluate_and_plot(model11, train_losses11, val_losses11, train_accuracies11, val_accuracies11, test_loader, test_dataset, device11)

"""Model 12:


Training Hyperparameters:

>>Training Loss: 0.7231, Training Accuracy:  80.23%

>>Validation Loss: 1.6938,  Validation Accuracy: 42.55%

>>Test Accuracy: 42.55%

Optimizer Type: SGD.

Learning Rate (lr): 0.001



Number of Epochs: 50

Batch Size:64

Loss Function:CrossEntropy Loss for multi-class classification tasks.
"""

model12 = CNNR(num_classes=num_classes)

criterion12 = nn.CrossEntropyLoss()

#model is on the CPU
device12 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model12.to(device12) # Move model to the correct device before calling summary



optimizer12 = torch.optim.SGD(
    model12.parameters(),
    lr=0.001,

    weight_decay=1e-4
)

train_losses12, val_losses12, train_accuracies12, val_accuracies12 = train_model(
    model=model12,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion12,
    optimizer=optimizer12,
    device=device12,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_model12.pth'
)

evaluate_and_plot(model12, train_losses12, val_losses12, train_accuracies12, val_accuracies12, test_loader, test_dataset, device12)

"""################################################################################

MODELS WITH DROPOUTS
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class CNNRD(nn.Module):

    def __init__(self, num_classes=10, dropout=0.5):
        super(CNNRD, self).__init__()

        # 1. Convolutional Layer + ReLU + Max Pooling
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 2. Convolutional Layer + ReLU + Max Pooling
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 3. Convolutional Layer + ReLU
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.res_conv2to3 = nn.Conv2d(64, 128, kernel_size=1)

        # 4. Convolutional Layer + ReLU + Max Pooling
        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)
        self.bn4 = nn.BatchNorm2d(256)
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 5. Convolutional Layer + ReLU + Max Pooling
        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)
        self.bn5 = nn.BatchNorm2d(512)
        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Fully Connected Layers
        self.fc1 = nn.Linear(512 * 16 * 16, 2048)
        self.dropout1 = nn.Dropout(p=dropout)
        self.fc2 = nn.Linear(2048, num_classes)

    def forward(self, x):
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))

        residual = self.res_conv2to3(x)

        x = F.relu(self.bn3(self.conv3(x)))
        x = x + residual

        x = self.pool4(F.relu(self.bn4(self.conv4(x))))
        x = self.pool5(F.relu(self.bn5(self.conv5(x))))

        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = self.fc2(x)

        return x

import torch
import torch.nn as nn
import torch.nn.functional as F

class CNND(nn.Module):
    def __init__(self, num_classes=10, dropout=0.3):
        super(CNND, self).__init__()

        # 1. Conv Block
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 2. Conv Block
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 3. Conv Block
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 4. Conv Block
        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)
        self.bn4 = nn.BatchNorm2d(256)
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 5. Conv Block
        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)
        self.bn5 = nn.BatchNorm2d(512)
        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Fully Connected Layers
        self.fc1 = nn.Linear(512 * 8 * 8, 512)
        self.dropout = nn.Dropout(p=dropout)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))
        x = self.pool3(F.relu(self.bn3(self.conv3(x))))
        x = self.pool4(F.relu(self.bn4(self.conv4(x))))
        x = self.pool5(F.relu(self.bn5(self.conv5(x))))

        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        return x

"""The Best Model with none residual model 2:

Model 2:

Training Hyperparameters:

>>Training Loss: 0.3547, Training Accuracy: 89.86%

>>Validation Loss: 2.2086, Validation Accuracy:45.45%

>>Test Accuracy: 38.18%


Optimizer Type: SGD.

Learning Rate (lr): 0.01



Number of Epochs: 50

Batch Size:32

Loss Function:CrossEntropyLoss for multi-class classification tasks.

dropout : 0.3

"""

modeld1 = CNND(num_classes=num_classes, dropout=0.3)

criteriond1 = nn.CrossEntropyLoss()

#model is on the CPU
deviced1 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
modeld1.to(deviced1)



optimizerd1 = torch.optim.SGD(
    modeld1.parameters(),
    lr=0.01,

    weight_decay=1e-4
)

train_lossesd1, val_lossesd1, train_accuraciesd1, val_accuraciesd1 = train_model(
    model=modeld1,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criteriond1,
    optimizer=optimizerd1,
    device=deviced1,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_modeld1.pth'
)

evaluate_and_plot(modeld1, train_lossesd1, val_lossesd1, train_accuraciesd1, val_accuraciesd1, test_loader, test_dataset, deviced1)

"""The Best Model with none residual model 2:

Model 2:

Training Hyperparameters:

>>Training Loss: 0.7109, Training Accuracy: 76.68%

>>Validation Loss: 1.6003,  Validation Accuracy: 53.09%

>>Test Accuracy: 45.82%


Optimizer Type: SGD.

Learning Rate (lr): 0.01



Number of Epochs: 50

Batch Size:32

Loss Function:CrossEntropyLoss for multi-class classification tasks.

dropout : 0.5

"""

modeld2 = CNND(num_classes=num_classes, dropout=0.5)

criteriond2 = nn.CrossEntropyLoss()

#model is on the CPU
deviced2 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
modeld2.to(deviced2)



optimizerd2 = torch.optim.SGD(
    modeld2.parameters(),
    lr=0.01,

    weight_decay=1e-4
)

train_lossesd2, val_lossesd2, train_accuraciesd2, val_accuraciesd2 = train_model(
    model=modeld2,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criteriond2,
    optimizer=optimizerd2,
    device=deviced2,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_modeld2.pth'
)

evaluate_and_plot(modeld2, train_lossesd2, val_lossesd2, train_accuraciesd2, val_accuraciesd2, test_loader, test_dataset, deviced2)

"""The Best Model with  residual model5

Model 5:

Training Hyperparameters:

>>Training Loss:0.1559, Training Accuracy: 97.18%

>>Validation Loss:  1.7111,   Validation Accuracy:49.09%

>>Test Accuracy: 43.64%

Optimizer Type: SGD.

Learning Rate (lr): 0.005



Number of Epochs: 50

Batch Size:32

Loss Function:CrossEntropyLoss for multi-class classification tasks.

dropout: 0.3
"""

modelrd1 = CNNRD(num_classes=num_classes, dropout=0.3)

criterionrd1 = nn.CrossEntropyLoss()

#model is on the CPU
devicerd1 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
modelrd1.to(devicerd1) # Move model to the correct device before calling summary



optimizerrd1 = torch.optim.SGD(
    modelrd1.parameters(),
    lr=0.005,

    weight_decay=1e-4
)

train_lossesrd1, val_lossesrd1, train_accuraciesrd1, val_accuraciesrd1 = train_model(
    model=modelrd1,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterionrd1,
    optimizer=optimizerrd1,
    device=devicerd1,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_modelrd1.pth'
)

evaluate_and_plot(modelrd1, train_lossesrd1, val_lossesrd1, train_accuraciesrd1, val_accuraciesrd1, test_loader, test_dataset, devicerd1)

"""The Best Model with  residual model5

Model 5:

Training Hyperparameters:

>>Training Loss:  0.5242, Training Accuracy:83.64%

>>Validation Loss:1.9648,  Validation Accuracy:43.27%

>>Test Accuracy: 37.45%

Optimizer Type: SGD.

Learning Rate (lr): 0.005



Number of Epochs: 50

Batch Size:32

Loss Function:CrossEntropyLoss for multi-class classification tasks.

dropout: 0.5
"""

modelrd2 = CNNRD(num_classes=num_classes, dropout=0.5)

criterionrd2 = nn.CrossEntropyLoss()

#model is on the CPU
devicerd2 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
modelrd2.to(devicerd2)

optimizerrd2 = torch.optim.SGD(
    modelrd2.parameters(),
    lr=0.005,

    weight_decay=1e-4
)

train_lossesrd2, val_lossesrd2, train_accuraciesrd2, val_accuraciesrd2 = train_model(
    model=modelrd2,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterionrd2,
    optimizer=optimizerrd2,
    device=devicerd2,
    num_epochs=50,
    save_path='/content/drive/MyDrive/CNN_modelrd2.pth'
)

evaluate_and_plot(modelrd2, train_lossesrd2, val_lossesrd2, train_accuraciesrd2, val_accuraciesrd2, test_loader, test_dataset, devicerd2)

"""
### PART 2 - Transfer Learning with CNNs"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import copy

num_classes = 11
num_epochs = 50
learning_rate = 0.01
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



def evaluate(model, loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    return correct / total


def train_model(model, train_loader, val_loader, trainable_params):
    model = model.to(device)
    optimizer = optim.SGD(trainable_params, lr=learning_rate, momentum=0.9, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        val_acc = evaluate(model, val_loader)
        print(f"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f} - Val Accuracy: {val_acc:.4f}")

    return model



def prepare_model(train_last_blocks=False):
    model = models.shufflenet_v2_x1_0(pretrained=True)

    # Bütün katmanları dondur
    for param in model.parameters():
        param.requires_grad = False

    # FC katmanını değiştir
    model.fc = nn.Linear(model.fc.in_features, num_classes)

    # Eğer son iki konv blok da eğitilecekse
    if train_last_blocks:
        for name, param in model.named_parameters():
            if "stage3" in name or "stage4" in name or "conv5" in name:
                param.requires_grad = True

    # Sadece eğitilecek parametreleri filtrele
    trainable_params = filter(lambda p: p.requires_grad, model.parameters())
    return model, trainable_params



print("\n Training CASE A: Only FC Layer")
model_fc_only, trainable_params_A = prepare_model(train_last_blocks=False)
model_fc_only = train_model(model_fc_only, train_loader, val_loader, trainable_params_A)

test_acc_A = evaluate(model_fc_only, test_loader)
print(f"\n Test Accuracy (Only FC): {test_acc_A:.4f}")

# Confusion Matrix
model_fc_only.eval()
all_preds, all_labels = [], []
with torch.no_grad():
    for inputs, labels in test_loader:
        inputs = inputs.to(device)
        outputs = model_fc_only(inputs)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.numpy())

cm_A = confusion_matrix(all_labels, all_preds)



print("\n Training CASE B: FC + Last 2 Conv Blocks")
model_fc_and_blocks, trainable_params_B = prepare_model(train_last_blocks=True)
model_fc_and_blocks = train_model(model_fc_and_blocks, train_loader, val_loader, trainable_params_B)

test_acc_B = evaluate(model_fc_and_blocks, test_loader)
print(f"\n Test Accuracy (FC + Last 2 Blocks): {test_acc_B:.4f}")

# Confusion Matrix
model_fc_and_blocks.eval()
all_preds, all_labels = [], []
with torch.no_grad():
    for inputs, labels in test_loader:
        inputs = inputs.to(device)
        outputs = model_fc_and_blocks(inputs)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.numpy())

cm_B = confusion_matrix(all_labels, all_preds)


fig, axes = plt.subplots(1, 2, figsize=(14, 6))
disp_A = ConfusionMatrixDisplay(confusion_matrix=cm_A, display_labels=train_dataset.classes)
disp_B = ConfusionMatrixDisplay(confusion_matrix=cm_B, display_labels=train_dataset.classes)

disp_A.plot(ax=axes[0], xticks_rotation=45)
axes[0].set_title("Only FC Layer")

disp_B.plot(ax=axes[1], xticks_rotation=45)
axes[1].set_title("FC + Last 2 Conv Blocks")

plt.tight_layout()
plt.show()